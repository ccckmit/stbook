+ 簡介


最大熵法則 (Maximum Entropy) 在自然語言與機器翻譯領域上相當有用，舉例而言，在統計式機器翻譯領域，最大熵法則就常被用在雙語語句的詞彙對齊問題上，並且有許多人以該法則實作出自動對齊軟體，而且效果不錯。

舉例而言，英文詞彙 fly 翻譯成中文時可能有『飛行、搭機、蒼蠅』等三種可能譯文，假如沒有其他種譯法，那我們就可以假設

> P(飛行 | fly) + (P(搭機 | fly) + P(蒼蠅 | fly) = 1

但是，滿足這個條件的機率分布有很多，像是平均分布 (各 1/3)，或者極端分布 P(飛行 | fly) = 1, 其餘為 0 等等，這些各式各樣的可能分布稱為一個機率模型 (Model)。

但是，什麼樣的分布最好呢？通常，在沒有任何進一步訊息的情況下，我們會傾向於使用平均分布。但是，如果有進一步的訊息，例如 加上限制條件 P(飛行 | fly) + (P(搭機 | fly) = 4/5。那麼，我們會傾向於使用哪種分布呢？

在資訊理論當中，當一個詞彙 w 的出現機率為 P(w) 時，其資訊量定義為如下公式。

[[math eq1]]
h(w) = - p_w log(p_w)
[[/math]]

對於一個語言的所有詞彙集合 (詞集) 而言，整個詞集平均的詞彙編碼長度，稱為該詞集的『熵』或亂度，定義為如下公式。

[[math eq2]]
H(W) = - \sum_{w \in W} p_w log(p_w)
[[/math]]

在數學符號的使用上，我們通常使用大寫代表整個隨機變數 (像是 W) 或整體函數 (像是 H)，小寫代表一個案例 (像是 w) 或案例的函數 (像是 h)。

+ 從觀察樣本到機率模型

針對英漢機器翻譯 (Machine Translation) 領域而言，假如訓練語料庫為一個很長的 (英文, 中文) 配對 [[$ (x_1, y_1) (x_2, y_2) ... (x_N, y_N) $]] 的雙語對齊語料庫。那麼，我們就可以統計配對 (x,y) 的出現機率 [[$ \tilde{p}(x,y) $]]，其算法如下。

[[math eq3]]
\tilde{p}(x,y) = \frac{n(x, y)}{N}
[[/math]]

其中的 n(x,y) 代表 (x,y) 配對出現的次數。

假如我們現在加入一個限制函數 f(x,y) 代表雙連詞 (bigram) 模型，如果 y 緊跟在 x 之後出現則為 1 ，否則為 0，其函數定義如下。

[[code]]
f(x,y) = 1 if y follow x
         0 otherwise
[[/code]]

那麼，f 函數在 P(X,Y) 分布下的期望值將可由下列公式定義之。

[[math eq4]]
\tilde{p}(f) \equiv \sum_{x,y} \tilde{p}(x,y) f(x,y)
[[/math]]

像 f 這樣的函數稱為特徵函數 (feature function，簡稱 feature)。

當我們取得一些統計上較為堅實的樣本，像是詞彙的出現率統計 [[$ \tlide{P}(X) $]] 時，我們可以假設真實的分布也應該與樣本一致，於是可以用下列算式取代 [[eref eq4]]。

[[math eq5]]
p(f) \equiv \sum_{x,y} \tilde{p}(x) p(y|x) f(x,y)
[[/math]]


[[math eq6]]
p(f) \equiv \tilde{p}(f)
[[/math]]

條件式 [[eref eq6]] 稱為一種限制 (Constraint)，這種限制要求我們只考慮符合這個條件的機率模型。於是我們可以將 [[eref eq4]], [eref eq5]], [[eref eq6]] 合併起來，形成下列等式。

[[math eq7]]
\sum_{x,y} \tilde{p}(x,y) f(x,y) = \sum_{x,y} \tilde{p}(x) p(y|x) f(x,y)
[[/math]]


也就是我們要求找出的機率模型必須與訓練樣本一致，此時我們可以將

 [[$ \tlide{P}(x,y) $]] 時 (像是雙語對齊語料庫)，

我們的目標是希望建立一個統計模型，讓這個模型產生整個機率分布 P(X,Y) 的機會最大，

當沒有進一步的訊息時，我們傾向於使用平均分布，也就是讓

+ 熵的距離公式

[[math]]
d(X,Y) = H(X,Y) - I(X;Y) = H(X|Y) + H(Y|X) = 2 H(X,Y) - H(X) - H(Y)
[[/math]]

+ 目標

最佳化目標：找出 p 以最大化下列算式
[[math type="eqnarray"]]
&& argmax \quad { d(X,Y) } \\
& \rightarrow & argmax \quad { H(X|Y) + H(Y|X)} \\
& \rightarrow & argmax \quad { 2 H(X,Y) - H(X) - H(Y) }
[[/math]]

最大距離原則 = 最大條件熵原則 = 最大聯合熵原則 (保持系統的最大亂度)



+ 參考文獻
# Maximum Entropy Modeling -- http://homepages.inf.ed.ac.uk/lzhang10/maxent.html

+ 軟體程式
# [http://www.fjoch.com/YASMET.html YASMET] -- training of conditional maximum entropy models, Yet Another Small MaxEnt Toolkit. Believe it or not, this implementation is written in only 132 lines of C++ code and still has feature selection and gaussian smoothing. You need GCC 2.9x to compile the source.
# [http://maxent.sf.net/ maxent.sf.net] -- Great java maxent implementation with GIS training algorithm. Part of OpenNlp project.
# [http://www-tsujii.is.s.u-tokyo.ac.jp/~yusuke/amis/ Amis] -- A maximum entropy estimator for feature forests. A maximum entropy estimator with GIS, IIS and L-BFGS algorithms.
# [http://www2.nict.go.jp/jt/a132/members/mutiyama/software.html#maxent maxent] -- Another Maximum Entropy Modeling Package with Ruby binding, GIS, Gaussian Prior smoothing and XML data format.
# [http://www.cs.princeton.edu/~ristad/papers/memt.html Predictive Modeling Toolkit]
# Robert Malouf's [Maximum Entropy Parameter Estimation software], now available as [Toolkit for Advanced Discriminative Modeling on sourceforge.net], has GIS, IIS, L-BFGS and Gradient Descent training methods and parallel computation ability through PETSc. You may want to read his paper first.
# [http://www.isi.edu/~hdaume/megam/ MEGA Model Optimization Package] -- A recently appeared ME implementation by Hal Daumé III. The software features CG and LM-BFGS Optimization and is written in OCaml. Although I no longer use OCaml, I'd say that's a great language, and is worth learning.
# [http://textmodeller.sourceforge.net/ Text Modeller] -- A python implementation of a joint Maximum Entropy model (aka. Whole Sentence Language Model) with sampling based training. Now seems to be part of scipy.
# [http://nlp.stanford.edu/downloads/classifier.shtml Stanford Classifer] is another open source implementation of Maximum Entropy Model in java, suitable for NLP tagging and parsing tasks.
# [http://nltk.sourceforge.net/ NLTK] includes a maxent classifier written entirely in Python. IIS and GIS training methods available. Suitable for text categorization and related NLP tasks.
# [http://www.cs.ualberta.ca/~lindek/downloads.htm Here] is another small maxent package in C++ with a BSD-like license, written by Dekang Lin.
# [http://www.thecodeproject.com/useritems/sharpentropy.asp SharpEntropy], a C# port of the java maxent package (http://maxent.sf.net) mentioned above.
# [http://www.cs.princeton.edu/~schapire/maxent/ Maxent software for species habitat modeling] by Robert E. Schapire et al. Registration needed for downloading.
# [http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html Maxent implementation in C++ with Python binding, GIS, L-BFGS and Gaussian Prior Smoothing]